{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0fd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd30f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession \n",
    "spark = SparkSession.builder.appName(\"LifeExpectation\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0518d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"data/Life_Expectancy_Data3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3d3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+----+---------------+-------------+-------+----------------------+----------+-------+----+-----------------+-----+-----------------+----------+--------+---------+-----------+-------------------+------------------+----------------------------+---------+------+\n",
      "|    Country|Year|    Status|Life|Adult_Mortality|infant_deaths|Alcohol|percentage_expenditure|HepatitisB|Measles| BMI|under-five_deaths|Polio|Total_expenditure|Diphtheria|HIV_AIDS|      GDP| Population|thinness_1-19_years|thinness_5-9_years|Income_composition_resources|Schooling|Region|\n",
      "+-----------+----+----------+----+---------------+-------------+-------+----------------------+----------+-------+----+-----------------+-----+-----------------+----------+--------+---------+-----------+-------------------+------------------+----------------------------+---------+------+\n",
      "|Afghanistan|2015|Developing|65.0|            263|           62|   0.01|           71.27962362|        65|   1154|19.1|               83|    6|             8.16|        65|     0.1|584.25921|3.3736494E7|               17.2|              17.3|                       0.479|     10.1|  Asia|\n",
      "+-----------+----+----------+----+---------------+-------------+-------+----------------------+----------+-------+----+-----------------+-----+-----------------+----------+--------+---------+-----------+-------------------+------------------+----------------------------+---------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5ffd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Life: double (nullable = true)\n",
      " |-- Adult_Mortality: integer (nullable = true)\n",
      " |-- infant_deaths: integer (nullable = true)\n",
      " |-- Alcohol: double (nullable = true)\n",
      " |-- percentage_expenditure: double (nullable = true)\n",
      " |-- HepatitisB: integer (nullable = true)\n",
      " |-- Measles: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- under-five_deaths: integer (nullable = true)\n",
      " |-- Polio: integer (nullable = true)\n",
      " |-- Total_expenditure: double (nullable = true)\n",
      " |-- Diphtheria: integer (nullable = true)\n",
      " |-- HIV_AIDS: double (nullable = true)\n",
      " |-- GDP: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- thinness_1-19_years: double (nullable = true)\n",
      " |-- thinness_5-9_years: double (nullable = true)\n",
      " |-- Income_composition_resources: double (nullable = true)\n",
      " |-- Schooling: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1698c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Dataset REMOVE Two Columns\n",
    "df = df.drop(*['Year','Country'])             # dropping some irrelevant columns\n",
    "\n",
    "#df = df.na.drop()\n",
    "# df = df.dropna(how = 'any', subset = ['userId', 'sessionId'])                   # droppping some potential NA values\n",
    "#df = df.filter(df.userId!='').orderBy([\"userId\", \"ts\"], ascending=[True, True]) # filtering out the invalid Ids\n",
    "#df = df.withColumn(\"userId\", df[\"userId\"].cast(IntegerType()))                  # case userId column to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9808f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTER\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=df.drop('Status','Region').columns,\n",
    "                 outputCols=[\"{}\".format(c) for c in df.drop('Status','Region').columns]).setStrategy('mean')\n",
    "\n",
    "# add imputation to cols\n",
    "df_imputed = imputer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6de125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Status',\n",
       " 'Life',\n",
       " 'Adult_Mortality',\n",
       " 'infant_deaths',\n",
       " 'Alcohol',\n",
       " 'percentage_expenditure',\n",
       " 'HepatitisB',\n",
       " 'Measles',\n",
       " 'BMI',\n",
       " 'under-five_deaths',\n",
       " 'Polio',\n",
       " 'Total_expenditure',\n",
       " 'Diphtheria',\n",
       " 'HIV_AIDS',\n",
       " 'GDP',\n",
       " 'Population',\n",
       " 'thinness_1-19_years',\n",
       " 'thinness_5-9_years',\n",
       " 'Income_composition_resources',\n",
       " 'Schooling',\n",
       " 'Region']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acebbe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------------+-------------+-------+----------------------+----------+-------+---+-----------------+-----+-----------------+----------+--------+---+----------+-------------------+------------------+----------------------------+---------+------+\n",
      "|Status|Life|Adult_Mortality|infant_deaths|Alcohol|percentage_expenditure|HepatitisB|Measles|BMI|under-five_deaths|Polio|Total_expenditure|Diphtheria|HIV_AIDS|GDP|Population|thinness_1-19_years|thinness_5-9_years|Income_composition_resources|Schooling|Region|\n",
      "+------+----+---------------+-------------+-------+----------------------+----------+-------+---+-----------------+-----+-----------------+----------+--------+---+----------+-------------------+------------------+----------------------------+---------+------+\n",
      "|     0|   0|              0|            0|      0|                     0|         0|      0|  0|                0|    0|                0|         0|       0|  0|         0|                  0|                 0|                           0|        0|     0|\n",
      "+------+----+---------------+-------------+-------+----------------------+----------+-------+---+-----------------+-----+-----------------+----------+--------+---+----------+-------------------+------------------+----------------------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "df_imputed.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_imputed.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff335554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb160a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life',\n",
       " 'Adult_Mortality',\n",
       " 'infant_deaths',\n",
       " 'Alcohol',\n",
       " 'percentage_expenditure',\n",
       " 'HepatitisB',\n",
       " 'Measles',\n",
       " 'BMI',\n",
       " 'under-five_deaths',\n",
       " 'Polio',\n",
       " 'Total_expenditure',\n",
       " 'Diphtheria',\n",
       " 'HIV_AIDS',\n",
       " 'GDP',\n",
       " 'Population',\n",
       " 'thinness_1-19_years',\n",
       " 'thinness_5-9_years',\n",
       " 'Income_composition_resources',\n",
       " 'Schooling',\n",
       " 'Status_Numeric',\n",
       " 'Region_onehot',\n",
       " 'Status_onehot']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot enconde Status and Region\n",
    "\n",
    "# First we index it\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCols=['Region','Status'], outputCols=['Region_Numeric','Status_Numeric'])\n",
    "indexer_fitted = indexer.fit(df_imputed)\n",
    "df_indexed = indexer_fitted.transform(df_imputed)\n",
    "\n",
    "#now we can one hot encode\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=['Region_Numeric','Status_Numeric'], outputCols=['Region_onehot','Status_onehot'])\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "#Remobe the categorical columns not needed anymore\n",
    "df_onehot = df_onehot.drop(*['Status','Region','Region_Numeric','Status_Nhmeric'])\n",
    "df_onehot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1b3a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Life: double (nullable = true)\n",
      " |-- Adult_Mortality: integer (nullable = true)\n",
      " |-- infant_deaths: integer (nullable = true)\n",
      " |-- Alcohol: double (nullable = true)\n",
      " |-- percentage_expenditure: double (nullable = true)\n",
      " |-- HepatitisB: integer (nullable = true)\n",
      " |-- Measles: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- under-five_deaths: integer (nullable = true)\n",
      " |-- Polio: integer (nullable = true)\n",
      " |-- Total_expenditure: double (nullable = true)\n",
      " |-- Diphtheria: integer (nullable = true)\n",
      " |-- HIV_AIDS: double (nullable = true)\n",
      " |-- GDP: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- thinness_1-19_years: double (nullable = true)\n",
      " |-- thinness_5-9_years: double (nullable = true)\n",
      " |-- Income_composition_resources: double (nullable = true)\n",
      " |-- Schooling: double (nullable = true)\n",
      " |-- Status_Numeric: double (nullable = false)\n",
      " |-- Region_onehot: vector (nullable = true)\n",
      " |-- Status_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce09ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = df_onehot.drop('Life').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c9c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "\n",
    "df_transform = assembler.transform(df_onehot).select(\"Life\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9bbb829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|Life|            features|\n",
      "+----+--------------------+\n",
      "|65.0|[263.0,62.0,0.01,...|\n",
      "|59.9|[271.0,64.0,0.01,...|\n",
      "|59.9|[268.0,66.0,0.01,...|\n",
      "|59.5|[272.0,69.0,0.01,...|\n",
      "|59.2|[275.0,71.0,0.01,...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transform.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539fc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "df_scaled = scaler.fit(df_transform).transform(df_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db77c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|Life|            features|     scaled_features|\n",
      "+----+--------------------+--------------------+\n",
      "|65.0|[263.0,62.0,0.01,...|[2.11959495798077...|\n",
      "|59.9|[271.0,64.0,0.01,...|[2.18406932932619...|\n",
      "|59.9|[268.0,66.0,0.01,...|[2.15989144007166...|\n",
      "|59.5|[272.0,69.0,0.01,...|[2.19212862574437...|\n",
      "|59.2|[275.0,71.0,0.01,...|[2.21630651499890...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c52b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our data into training data and testing data\n",
    "train,test = df_scaled.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a788d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "\n",
    "lr_model = LinearRegression(featuresCol='scaled_features', labelCol='Life')\n",
    "lr_fitted = lr_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d424ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.748937\n",
      "r2: 0.843096\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_fitted.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfa8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05017671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create our decision tree\n",
    "dtr = DecisionTreeRegressor().setFeaturesCol(\"scaled_features\").setLabelCol(\"Life\")\n",
    "\n",
    "# Train the model using our training data\n",
    "tree_model = dtr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8496f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see if we can predict values in our test data.\n",
    "# Generate predictions using our decision tree model for all features in our\n",
    "# test dataframe:\n",
    "fullPredictions = tree_model.transform(test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6aff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+------------------+\n",
      "|Life|            features|     scaled_features|        prediction|\n",
      "+----+--------------------+--------------------+------------------+\n",
      "|43.3|[48.0,30.0,3.83,4...|[0.38684622807253...|56.705729166666664|\n",
      "|43.5|[599.0,48.0,1.15,...|[4.82751855448852...|          46.34375|\n",
      "|44.0|[67.0,46.0,1.1,3....|[0.53997286001791...|56.705729166666664|\n",
      "|44.5|[675.0,5.0,2.67,5...|[5.44002508227004...|          46.34375|\n",
      "|44.5|[715.0,26.0,4.06,...|[5.76239693899715...|          46.34375|\n",
      "|44.8|[73.0,25.0,4.43,0...|[0.58832863852698...|56.705729166666664|\n",
      "|45.3|[593.0,7.0,0.83,0...|[4.77916277597945...|          46.34375|\n",
      "|45.5|[69.0,41.0,2.44,5...|[0.55609145285427...|56.705729166666664|\n",
      "|45.6|[54.0,17.0,1.52,3...|[0.43520200658160...|51.816326530612244|\n",
      "|45.6|[69.0,3.0,5.78,37...|[0.55609145285427...|58.745918367346924|\n",
      "|45.9|[511.0,17.0,1.5,4...|[4.11830046968887...|          46.34375|\n",
      "|46.0|[63.0,3.0,5.08,37...|[0.50773567434520...|58.745918367346924|\n",
      "|46.2|[441.0,29.0,3.91,...|[3.55414972041642...| 51.46976744186046|\n",
      "|46.5|[391.0,96.0,2.82,...|[3.15118489950753...| 51.46976744186046|\n",
      "|47.8|[592.0,5.0,2.75,9...|[4.77110347956128...| 51.04999999999999|\n",
      "|47.8|[647.0,2.0,5.37,2...|[5.21436478256106...| 51.04999999999999|\n",
      "|47.9|[461.0,67.0,3.45,...|[3.71533564877998...| 51.04999999999999|\n",
      "|48.0|[4.0,42.0,0.3,26....|[0.03223718567271...|51.816326530612244|\n",
      "|48.1|[43.0,43.0,0.36,3...|[0.34654974598164...|51.816326530612244|\n",
      "|48.1|[424.0,27.0,3.84,...|[3.41714168130740...| 51.46976744186046|\n",
      "+----+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullPredictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfeb863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.884091\n"
     ]
    }
   ],
   "source": [
    "dt_evaluator = RegressionEvaluator(labelCol=\"Life\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = dt_evaluator.evaluate(fullPredictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2314a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daabdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression(featuresCol='scaled_features', labelCol='Life')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d98dc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline1=Pipeline(stages=[assembler, scaler, lr])\n",
    "pipeline1=Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e440b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8420121042520536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramgrid =ParamGridBuilder()\\\n",
    ".addGrid(lr.regParam, [0, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\\\n",
    ".addGrid(lr.maxIter, [10])\\\n",
    ".build()\n",
    "\n",
    "\n",
    "evaluator=RegressionEvaluator(metricName=\"r2\").setLabelCol(\"Life\")\n",
    "\n",
    "\n",
    "crossval= CrossValidator(estimator=pipeline1,  \n",
    "                         estimatorParamMaps=paramgrid,\n",
    "                         evaluator = evaluator, \n",
    "                         numFolds=5\n",
    "                        )\n",
    "\n",
    "cvModel1=crossval.fit(train) \n",
    "evaluator.evaluate(cvModel1.transform(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba78311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_845e365a736a', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       " Param(parent='LinearRegression_845e365a736a', name='maxIter', doc='max number of iterations (>= 0).'): 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel1.getEstimatorParamMaps()[ np.argmax(cvModel1.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c6aa126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8348072739048387,\n",
       " 0.8358324270001516,\n",
       " 0.835761267076788,\n",
       " 0.8351171981645227,\n",
       " 0.8350327892182114,\n",
       " 0.834507277236124,\n",
       " 0.8322460374614746]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel1.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac3c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d198448b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9601563822827419"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol='scaled_features', labelCol='Life')\n",
    "pipeline2 = Pipeline(stages=[rf])\n",
    "\n",
    "paramgrid =ParamGridBuilder()\\\n",
    ".addGrid(rf.numTrees, [200, 500])\\\n",
    ".addGrid(rf.maxDepth, [5,10])\\\n",
    ".build()\n",
    "\n",
    "\n",
    "evaluator=RegressionEvaluator(metricName=\"r2\").setLabelCol(\"Life\")\n",
    "\n",
    "\n",
    "crossval= CrossValidator(estimator=pipeline2,  \n",
    "                         estimatorParamMaps=paramgrid,\n",
    "                         evaluator = evaluator, \n",
    "                         numFolds=5\n",
    "                        )\n",
    "\n",
    "cvModel2=crossval.fit(train) \n",
    "\n",
    "evaluator.evaluate(cvModel2.transform(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "016016c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestRegressor_1d0f01b4213a', name='numTrees', doc='Number of trees to train (>= 1).'): 500,\n",
       " Param(parent='RandomForestRegressor_1d0f01b4213a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel2.getEstimatorParamMaps()[ np.argmax(cvModel2.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc4fca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd5da283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9406648503167245"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt = GBTRegressor(featuresCol='scaled_features', labelCol='Life')\n",
    "pipeline3 = Pipeline(stages=[gbt])\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5, 10])\\\n",
    "  .addGrid(gbt.maxIter, [10,100,500])\\\n",
    "  .build()\n",
    "\n",
    "evaluator=RegressionEvaluator(metricName=\"r2\").setLabelCol(\"Life\")\n",
    "\n",
    "\n",
    "crossval= CrossValidator(estimator=pipeline3,  \n",
    "                         estimatorParamMaps=paramgrid,\n",
    "                         evaluator = evaluator, \n",
    "                         numFolds=5\n",
    "                        )\n",
    "\n",
    "cvModel3=crossval.fit(train) \n",
    "\n",
    "evaluator.evaluate(cvModel3.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ec3adce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##################################################################################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#   From Site\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train Test Split\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m training, test \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make VectorAssembler - this is a Pypark specific step\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# All input features must be in one column before feeding into the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msong_count\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriends\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaylist_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \\\n\u001b[0;32m     15\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthumbs_up\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthumbs_down\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdowngrade\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_session_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\\\n\u001b[0;32m     16\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_diff_time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpages_per_session\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_shift\u001b[39m\u001b[38;5;124m\"\u001b[39m,\\\n\u001b[0;32m     17\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], \\\n\u001b[0;32m     18\u001b[0m                             outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "#\n",
    "#   From Site\n",
    "#\n",
    "##################################################################################################################\n",
    "\n",
    "# Train Test Split\n",
    "training, test = features.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "# Make VectorAssembler - this is a Pypark specific step\n",
    "# All input features must be in one column before feeding into the model\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"userId\",\"song_count\",\"error\",\"friends\",\"playlist_count\", \\\n",
    "                                       \"thumbs_up\",\"thumbs_down\",\"downgrade\", \"count_session_dist\",\\\n",
    "                                       \"count_diff_time\",\"pages_per_session\", \"duration\",\"level_shift\",\\\n",
    "                                       \"usage_time\"], \\\n",
    "                            outputCol=\"inputFeatures\")\n",
    "\n",
    "\n",
    "\n",
    "# Normalize Data\n",
    "scaler = Normalizer(inputCol=\"inputFeatures\", outputCol=\"features\")\n",
    "\n",
    "\n",
    "# Spark supports most common classification methods (https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "# I decided for the following three:\n",
    "\n",
    "lr=LogisticRegression()\n",
    "gbt = GBTClassifier()\n",
    "rf= RandomForestClassifier()\n",
    "\n",
    "\n",
    "# Building pipelines\n",
    "pipeline1=Pipeline(stages=[assembler, scaler, lr])\n",
    "pipeline2=Pipeline(stages=[assembler, scaler, gbt])\n",
    "pipeline3=Pipeline(stages=[assembler, scaler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8282a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark_env]",
   "language": "python",
   "name": "conda-env-pyspark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
